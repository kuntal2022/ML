{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba85cc5",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated\n",
    "\n",
    "\n",
    "# ANS\n",
    "\n",
    "## Overfit:\n",
    "\n",
    "When the model read each data points in detail and return a high accuracy for train set but low accuracy for test set that situtaion called overfitting.\n",
    "\n",
    "## Overfit models are \n",
    "### Low biase: \n",
    "> As the tranning accuracy is high\n",
    "\n",
    "### High Vraiance:\n",
    "\n",
    "> As the test accuracy is low\n",
    "\n",
    "\n",
    "\n",
    "## Underfitting:\n",
    "\n",
    "When the model does get enough data to trained uppon, the accuracy for train and test both are low.\n",
    "\n",
    "\n",
    "## Underfitted model are \n",
    "### High Variance and High Biase Model\n",
    "\n",
    "\n",
    "\n",
    "## The Overfitting or Underfitting can be mitigated by tarinning the model with optimum data, after providing optimum features to train uppon to over come the high variance and high biase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d6cbf",
   "metadata": {},
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brife\n",
    "\n",
    "\n",
    "# ANS:\n",
    "\n",
    "* We can reduce the overfitting the model by providing optimum features to train itself.\n",
    "* There are some methods like geting the feature's importance or we can get the optimum features by RFE method,regularization with laso, ridge, or VIF or by analysising the statistics of f value through stats model to understand which feature is having high correlation with target variable. \n",
    "\n",
    "\n",
    "* After getting the high correlated feature we can remove other less important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582d35f",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML\n",
    "\n",
    "# ANS\n",
    "### Underfitting occurs when a machine learning model is too simple or not complex enough to capture the underlying patterns and relationships in the training data. This results in the model being unable to learn the relevant patterns in the data and thus performs poorly on both the training and test data.\n",
    "\n",
    "* Underfitting can occur in machine learning in the following scenarios:\n",
    "\n",
    "* Insufficiently Complex Model: When a model is too simple to represent the complexity of the data, it may underfit the training data. For example, a linear regression model may underfit a dataset with complex non-linear relationships.\n",
    "\n",
    "* Insufficient Training Data: When the size of the training data is too small, the model may not have enough information to learn the patterns in the data. This may result in underfitting because the model has not seen enough examples to learn the underlying patterns.\n",
    "\n",
    "* Over-regularization: When the regularization parameter in a model is set too high, it can result in the model being too rigid and unable to capture the relevant patterns in the data. This can lead to underfitting, where the model does not learn the relevant patterns in the training data.\n",
    "\n",
    "* Outliers in Data: When the training data contains outliers or noise, the model may try to fit the noise instead of the underlying patterns in the data. This can result in underfitting, where the model is unable to capture the underlying patterns in the data due to being thrown off by the outliers.\n",
    "\n",
    "* Inappropriate Model Selection: When the wrong model is selected for a particular task, it can result in underfitting. For example, using a linear regression model for an image classification task may result in underfitting because the model is not complex enough to capture the patterns in the image data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47414a4d",
   "metadata": {},
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance\n",
    "\n",
    "\n",
    "# Biase: \n",
    "* When the model is giving high accuracy for Train set called low biase and if it gives low accuracy then the situatio is called high biase.\n",
    "\n",
    "# Variance:\n",
    "* When model gives low accuracy for test set called high Variance and if it give high accuracy for test set than it is called low variance\n",
    "\n",
    "\n",
    "# We need to make a balance to the model so it should nither be overfitted or underfitted but blancefitted.\n",
    "# Else the performance of the model will be low in the term of prediction of the result. By giving less accuracy to the proided new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2cb34",
   "metadata": {},
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "\n",
    "# ANS:\n",
    "\n",
    "## Cross-validation: \n",
    "Cross-validation is a technique used to estimate the performance of a model on new data by partitioning the data into training and validation sets. This can help detect overfitting by evaluating the performance of the model on the validation set, which is independent of the training set.\n",
    "\n",
    "## Learning curves: \n",
    "Learning curves plot the performance of a model on the training and validation sets as a function of the number of training examples. This can help detect overfitting and underfitting by showing whether the model is learning the patterns in the data or not.\n",
    "\n",
    "## Regularization: \n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the objective function. This encourages the model to learn simpler patterns that generalize well to new data.\n",
    "\n",
    "## Feature selection: \n",
    "Feature selection is a technique used to select the most relevant features for the model. This can help prevent overfitting by reducing the complexity of the model.\n",
    "\n",
    "## Ensemble methods: \n",
    "Ensemble methods combine multiple models to improve performance and reduce overfitting. For example, random forests and boosting algorithms are ensemble methods that combine multiple decision trees to improve performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770295e9",
   "metadata": {},
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance\n",
    "\n",
    "\n",
    "# ANS\n",
    "\n",
    "### Low biase:\n",
    "Model's tranning accuracy is high\n",
    "\n",
    "#### High Vraiance:\n",
    "Model's test accuracy is low\n",
    "\n",
    "* The performance of a model depends on the trade-off between bias and variance. A high bias model is underfitting the data and has poor performance on both the training and test sets. A high variance model is overfitting the data and has good performance on the training set but poor performance on the test set. The ideal model is one that has low bias and low variance, meaning it can fit the patterns in the data well and produce consistent predictions across different samples of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f155219",
   "metadata": {},
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work\n",
    "\n",
    "\n",
    "# ANS\n",
    "\n",
    "## Regularization\n",
    "* is a technique used in machine learning to prevent overfitting of models. Overfitting occurs when a model is too complex and captures noise in the training data, resulting in poor performance on new, unseen data. Regularization works by adding a penalty term to the objective function that the model is trying to optimize, which discourages the model from fitting the noise in the data.\n",
    "\n",
    "## The two most common types of regularization techniques are L1 regularization (also known as Lasso regularization) and L2 regularization (also known as Ridge regularization).\n",
    "\n",
    "## L1 regularization\n",
    "* adds a penalty term to the objective function that is proportional to the absolute value of the weights of the model. This encourages the model to set some of the weights to zero, resulting in a sparse model. L1 regularization is particularly effective when there are many features in the data and only a few of them are relevant for predicting the target variable.\n",
    "\n",
    "## L2 regularization\n",
    "* adds a penalty term to the objective function that is proportional to the square of the weights of the model. This encourages the model to have smaller weights and reduces the sensitivity of the model to changes in the data. L2 regularization is particularly effective when there is high correlation between the features in the data and when all the features are relevant for predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74090d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
